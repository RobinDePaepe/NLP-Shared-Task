{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "TC4OWeyBTnwr"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\robin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# transformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# pipeline\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ZQ2SMUwxTnws"
   },
   "outputs": [],
   "source": [
    "#load the data\n",
    "#toefl = pd.read_csv('Google Drive/NLP Shared task/toefl11_trainingdata_features') --> colab\n",
    "toefl = pd.read_csv('toefl11_trainingdata_features')\n",
    "toefl = toefl.drop(['Unnamed: 0','Unnamed: 0.1'], axis= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "b4MOssGCTnws"
   },
   "outputs": [],
   "source": [
    "#Pipeline\n",
    "#define features and y \n",
    "x = toefl.drop(['Filename', 'Language', 'Proficiency'], axis= 1)\n",
    "y = toefl.drop(['Filename','text','POS','DEP', 'NER', 'prop_punct', 'prop_capwords', 'prop_capI','avg_sentlength'], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "aI9TeZXtTnws"
   },
   "outputs": [],
   "source": [
    "#define custom tranformer\n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "        \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "EDeaYnyTTnwt"
   },
   "outputs": [],
   "source": [
    "numeric_features = ['prop_punct', 'avg_sentlength', 'prop_capwords', 'prop_capI']\n",
    "tfvect = CountVectorizer()\n",
    "tfidfvect = TfidfVectorizer()\n",
    "svm = SVC()\n",
    "linsvm = LinearSVC()\n",
    "#make the individual pipelines\n",
    "\n",
    "# pipe numeric features\n",
    "numfeat_pipe = Pipeline([\n",
    "    ('selector', ItemSelector(key=numeric_features))\n",
    "])\n",
    "\n",
    "# pipe word n-grams\n",
    "text_pipe = Pipeline([\n",
    "    ('selector', ItemSelector(key='text')),\n",
    "    ('vect', TfidfVectorizer(\n",
    "        tokenizer=word_tokenize,\n",
    "        analyzer='word',\n",
    "        lowercase=True,\n",
    "        ngram_range=(1,2)))\n",
    "])\n",
    "\n",
    "# pipe char n-grams\n",
    "char_pipe = Pipeline([\n",
    "    ('selector', ItemSelector(key=\"text\")),\n",
    "    ('vect', TfidfVectorizer(\n",
    "        analyzer='char',\n",
    "        lowercase=True,\n",
    "        ngram_range=(2,3)))\n",
    "])\n",
    "\n",
    "# pipe POS n-grams\n",
    "pos_pipe = Pipeline([\n",
    "    ('selector', ItemSelector(key=\"POS\")),\n",
    "    ('vect', TfidfVectorizer(\n",
    "        analyzer='word',\n",
    "        lowercase=True,\n",
    "        ngram_range=(1,3)))\n",
    "])\n",
    "\n",
    "# pipe dependencies\n",
    "dep_pipe = Pipeline([\n",
    "    ('selector', ItemSelector(key=\"DEP\")),\n",
    "    ('vect', TfidfVectorizer(\n",
    "        analyzer='word',\n",
    "        lowercase=True,\n",
    "        ngram_range=(1,2)))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "uZ0Uad-cTnwt"
   },
   "outputs": [],
   "source": [
    "# pipe complete\n",
    "pipe = Pipeline([\n",
    "    ('feats', FeatureUnion([\n",
    "        ('numfeat_pipe', numfeat_pipe),\n",
    "        ('text_pipe', text_pipe),\n",
    "        ('char_pipe', char_pipe),\n",
    "        ('pos_pipe', pos_pipe)\n",
    "    ])),\n",
    "    ('cls', linsvm)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "kcJIENExTnwt"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x, y['Language'], test_size= 0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "eN_NLTVnTnwt",
    "outputId": "ab2879a5-9086-4159-9f6a-d36d9573e62f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\robin\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('feats',\n",
       "                 FeatureUnion(transformer_list=[('numfeat_pipe',\n",
       "                                                 Pipeline(steps=[('selector',\n",
       "                                                                  ItemSelector(key=['prop_punct',\n",
       "                                                                                    'avg_sentlength',\n",
       "                                                                                    'prop_capwords',\n",
       "                                                                                    'prop_capI']))])),\n",
       "                                                ('text_pipe',\n",
       "                                                 Pipeline(steps=[('selector',\n",
       "                                                                  ItemSelector(key='text')),\n",
       "                                                                 ('vect',\n",
       "                                                                  TfidfVectorizer(ngram_range=(1,\n",
       "                                                                                               2),\n",
       "                                                                                  tokenizer=<function word_tokenize at 0x0000021DAA332820>))])),\n",
       "                                                ('char_pipe',\n",
       "                                                 Pipeline(steps=[('selector',\n",
       "                                                                  ItemSelector(key='text')),\n",
       "                                                                 ('vect',\n",
       "                                                                  TfidfVectorizer(analyzer='char',\n",
       "                                                                                  ngram_range=(2,\n",
       "                                                                                               3)))])),\n",
       "                                                ('pos_pipe',\n",
       "                                                 Pipeline(steps=[('selector',\n",
       "                                                                  ItemSelector(key='POS')),\n",
       "                                                                 ('vect',\n",
       "                                                                  TfidfVectorizer(ngram_range=(1,\n",
       "                                                                                               3)))]))])),\n",
       "                ('cls', LinearSVC())])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "HTHiXbmFTnwu",
    "outputId": "f74a7b04-e6cc-4d6f-ca63-afb063eeda51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ARA       0.88      0.75      0.81       103\n",
      "         DEU       0.86      0.95      0.91       101\n",
      "         FRA       0.86      0.77      0.81       115\n",
      "         HIN       0.77      0.79      0.78       111\n",
      "         ITA       0.74      0.93      0.83        92\n",
      "         JPN       0.82      0.72      0.77        98\n",
      "         KOR       0.82      0.80      0.81        98\n",
      "         SPA       0.73      0.71      0.72        96\n",
      "         TEL       0.81      0.81      0.81       100\n",
      "         TUR       0.81      0.84      0.82        89\n",
      "         ZHO       0.83      0.85      0.84        97\n",
      "\n",
      "    accuracy                           0.81      1100\n",
      "   macro avg       0.81      0.81      0.81      1100\n",
      "weighted avg       0.81      0.81      0.81      1100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# prediction\n",
    "y_test_pred = pipe.predict(X_test)\n",
    "\n",
    "# accuracy\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "rEqtIJMJTnwu",
    "outputId": "0ef8870f-2ae4-41fd-f562-ce9a19d45ce9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed: 32.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ARA       0.85      0.74      0.79      1000\n",
      "         DEU       0.85      0.93      0.89      1000\n",
      "         FRA       0.83      0.84      0.83      1000\n",
      "         HIN       0.70      0.76      0.73      1000\n",
      "         ITA       0.87      0.86      0.86      1000\n",
      "         JPN       0.82      0.79      0.81      1000\n",
      "         KOR       0.78      0.78      0.78      1000\n",
      "         SPA       0.76      0.78      0.77      1000\n",
      "         TEL       0.79      0.76      0.77      1000\n",
      "         TUR       0.86      0.86      0.86      1000\n",
      "         ZHO       0.83      0.87      0.85      1000\n",
      "\n",
      "    accuracy                           0.81     11000\n",
      "   macro avg       0.81      0.81      0.81     11000\n",
      "weighted avg       0.81      0.81      0.81     11000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# cross-validation\n",
    "cv_results = cross_val_predict(pipe, x, y['Language'], cv=10, verbose=True, n_jobs=-1)\n",
    "print(classification_report(y['Language'], cv_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u5p_-4gkTnwu",
    "outputId": "ce73b86c-8391-4d4e-8464-28045063c6bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed: 15.1min\n",
      "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed: 29.7min\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed: 50.1min\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed: 68.1min\n",
      "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed: 93.5min\n",
      "[Parallel(n_jobs=-1)]: Done  48 tasks      | elapsed: 122.3min\n",
      "[Parallel(n_jobs=-1)]: Done  61 tasks      | elapsed: 161.8min\n",
      "[Parallel(n_jobs=-1)]: Done  77 out of  90 | elapsed: 214.6min remaining: 36.2min\n",
      "[Parallel(n_jobs=-1)]: Done  87 out of  90 | elapsed: 252.4min remaining:  8.7min\n",
      "[Parallel(n_jobs=-1)]: Done  90 out of  90 | elapsed: 256.7min finished\n",
      "C:\\Users\\robin\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('feats',\n",
       "                                        FeatureUnion(transformer_list=[('numfeat_pipe',\n",
       "                                                                        Pipeline(steps=[('selector',\n",
       "                                                                                         ItemSelector(key=['prop_punct',\n",
       "                                                                                                           'avg_sentlength',\n",
       "                                                                                                           'prop_capwords',\n",
       "                                                                                                           'prop_capI']))])),\n",
       "                                                                       ('text_pipe',\n",
       "                                                                        Pipeline(steps=[('selector',\n",
       "                                                                                         ItemSelector(key='text')),\n",
       "                                                                                        ('vect',\n",
       "                                                                                         TfidfVectorizer(ngram_range=(1,\n",
       "                                                                                                                      2),\n",
       "                                                                                                         tokenizer=<function word_tokeniz...\n",
       "                                                                                                         ngram_range=(2,\n",
       "                                                                                                                      3)))])),\n",
       "                                                                       ('pos_pipe',\n",
       "                                                                        Pipeline(steps=[('selector',\n",
       "                                                                                         ItemSelector(key='POS')),\n",
       "                                                                                        ('vect',\n",
       "                                                                                         TfidfVectorizer(ngram_range=(1,\n",
       "                                                                                                                      3)))]))])),\n",
       "                                       ('cls', LinearSVC())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'feats__char_pipe__vect__ngram_range': [(2, 2), (2, 3),\n",
       "                                                                 (2, 4)],\n",
       "                         'feats__pos_pipe__vect__ngram_range': [(1, 2), (1, 3),\n",
       "                                                                (1, 4)],\n",
       "                         'feats__text_pipe__vect__ngram_range': [(1, 2),\n",
       "                                                                 (1, 3)]},\n",
       "             verbose=10)"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\"feats__text_pipe__vect__ngram_range\": [(1,2), (1,3)], \n",
    "          \"feats__char_pipe__vect__ngram_range\": [(2,2), (2,3), (2,4)],\n",
    "          \"feats__pos_pipe__vect__ngram_range\": [(1,2),(1,3),(1,4)]\n",
    "         }\n",
    "\n",
    "ngram_search = GridSearchCV(pipe, params, cv=5, n_jobs=-1, verbose=10)\n",
    "ngram_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Phcm_9NBTnwv",
    "outputId": "1db818e6-eba8-44ca-b0aa-5d969c794c4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7997727272727273\n",
      "{'feats__char_pipe__vect__ngram_range': (2, 4), 'feats__pos_pipe__vect__ngram_range': (1, 3), 'feats__text_pipe__vect__ngram_range': (1, 2)}\n"
     ]
    }
   ],
   "source": [
    "print(ngram_search.best_score_)\n",
    "print(ngram_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "ajVm57TPTnwv"
   },
   "outputs": [],
   "source": [
    "#refit with best ranges\n",
    "numfeat_pipe = Pipeline([\n",
    "    ('selector', ItemSelector(key=numeric_features))\n",
    "])\n",
    "\n",
    "# pipe word n-grams\n",
    "text_pipe = Pipeline([\n",
    "    ('selector', ItemSelector(key='text')),\n",
    "    ('vect', TfidfVectorizer(\n",
    "        tokenizer=word_tokenize,\n",
    "        analyzer='word',\n",
    "        lowercase=True,\n",
    "        ngram_range=(1,2)))\n",
    "])\n",
    "\n",
    "# pipe char n-grams\n",
    "char_pipe = Pipeline([\n",
    "    ('selector', ItemSelector(key=\"text\")),\n",
    "    ('vect', TfidfVectorizer(\n",
    "        analyzer='char',\n",
    "        lowercase=True,\n",
    "        ngram_range=(2,4)))\n",
    "])\n",
    "\n",
    "# pipe POS n-grams\n",
    "pos_pipe = Pipeline([\n",
    "    ('selector', ItemSelector(key=\"POS\")),\n",
    "    ('vect', TfidfVectorizer(\n",
    "        analyzer='word',\n",
    "        lowercase=True,\n",
    "        ngram_range=(1,3)))\n",
    "])\n",
    "\n",
    "# pipe complete\n",
    "pipe = Pipeline([\n",
    "    ('feats', FeatureUnion([\n",
    "        ('numfeat_pipe', numfeat_pipe),\n",
    "        ('text_pipe', text_pipe),\n",
    "        ('char_pipe', char_pipe),\n",
    "        ('pos_pipe', pos_pipe)\n",
    "    ])),\n",
    "    ('cls', linsvm)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "pa4kfMuaTnwv",
    "outputId": "788d731f-64b6-4fbf-982f-903827ee54ad"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\robin\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('feats',\n",
       "                 FeatureUnion(transformer_list=[('numfeat_pipe',\n",
       "                                                 Pipeline(steps=[('selector',\n",
       "                                                                  ItemSelector(key=['prop_punct',\n",
       "                                                                                    'avg_sentlength',\n",
       "                                                                                    'prop_capwords',\n",
       "                                                                                    'prop_capI']))])),\n",
       "                                                ('text_pipe',\n",
       "                                                 Pipeline(steps=[('selector',\n",
       "                                                                  ItemSelector(key='text')),\n",
       "                                                                 ('vect',\n",
       "                                                                  TfidfVectorizer(ngram_range=(1,\n",
       "                                                                                               2),\n",
       "                                                                                  tokenizer=<function word_tokenize at 0x0000021DAA332820>))])),\n",
       "                                                ('char_pipe',\n",
       "                                                 Pipeline(steps=[('selector',\n",
       "                                                                  ItemSelector(key='text')),\n",
       "                                                                 ('vect',\n",
       "                                                                  TfidfVectorizer(analyzer='char',\n",
       "                                                                                  ngram_range=(2,\n",
       "                                                                                               4)))])),\n",
       "                                                ('pos_pipe',\n",
       "                                                 Pipeline(steps=[('selector',\n",
       "                                                                  ItemSelector(key='POS')),\n",
       "                                                                 ('vect',\n",
       "                                                                  TfidfVectorizer(ngram_range=(1,\n",
       "                                                                                               3)))]))])),\n",
       "                ('cls', LinearSVC())])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "fqNMG7_MTnwv",
    "outputId": "16c87e8e-749a-40b5-eee3-de701e4a90bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ARA       0.87      0.82      0.85        97\n",
      "         DEU       0.85      0.97      0.91       115\n",
      "         FRA       0.90      0.85      0.87        98\n",
      "         HIN       0.79      0.71      0.75       120\n",
      "         ITA       0.86      0.92      0.89       101\n",
      "         JPN       0.87      0.88      0.88       100\n",
      "         KOR       0.89      0.80      0.84        95\n",
      "         SPA       0.78      0.77      0.77        90\n",
      "         TEL       0.74      0.78      0.76        91\n",
      "         TUR       0.86      0.91      0.89        98\n",
      "         ZHO       0.88      0.89      0.89        95\n",
      "\n",
      "    accuracy                           0.85      1100\n",
      "   macro avg       0.85      0.85      0.84      1100\n",
      "weighted avg       0.85      0.85      0.84      1100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# prediction\n",
    "y_test_pred = pipe.predict(X_test)\n",
    "\n",
    "# accuracy\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "id": "fyuXPwK0Tnww",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "1f207af6-d5fd-4b4c-8dda-1d618baf89a3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed: 41.8min finished\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [1100, 11000]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-5db8fce3fafd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# cross-validation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mcv_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_val_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpipe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Language'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36mclassification_report\u001b[1;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[0;32m   1927\u001b[0m     \"\"\"\n\u001b[0;32m   1928\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1929\u001b[1;33m     \u001b[0my_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1930\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1931\u001b[0m     \u001b[0mlabels_given\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0marray\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m     \"\"\"\n\u001b[1;32m---> 81\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    253\u001b[0m     \u001b[0muniques\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 255\u001b[1;33m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0m\u001b[0;32m    256\u001b[0m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [1100, 11000]"
     ]
    }
   ],
   "source": [
    "# cross-validation\n",
    "cv_results = cross_val_predict(pipe, x, y['Language'], cv=10, verbose=True, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ARA       0.83      0.77      0.80      1000\n",
      "         DEU       0.87      0.93      0.90      1000\n",
      "         FRA       0.85      0.83      0.84      1000\n",
      "         HIN       0.71      0.74      0.73      1000\n",
      "         ITA       0.87      0.87      0.87      1000\n",
      "         JPN       0.82      0.80      0.81      1000\n",
      "         KOR       0.79      0.79      0.79      1000\n",
      "         SPA       0.77      0.79      0.78      1000\n",
      "         TEL       0.78      0.77      0.78      1000\n",
      "         TUR       0.88      0.85      0.87      1000\n",
      "         ZHO       0.84      0.88      0.86      1000\n",
      "\n",
      "    accuracy                           0.82     11000\n",
      "   macro avg       0.82      0.82      0.82     11000\n",
      "weighted avg       0.82      0.82      0.82     11000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y['Language'], cv_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nSl1k4wmTnww"
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('Featured_Data/test_features.csv')\n",
    "test_predict = pipe.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dJ5NWnVhTnwx"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "3.1-TestModel_NLI.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
