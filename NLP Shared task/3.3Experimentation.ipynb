{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\robin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "\n",
    "# transformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# pipeline\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Google Drive/NLP Shared task/toefl11_trainingdata_features'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-2246aeeceda7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#load the data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtoefl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Google Drive/NLP Shared task/toefl11_trainingdata_features'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 452\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    945\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 946\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    947\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1176\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1177\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1178\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1179\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   2006\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2007\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2008\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2009\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2010\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Google Drive/NLP Shared task/toefl11_trainingdata_features'"
     ]
    }
   ],
   "source": [
    "#load the data\n",
    "toefl = pd.read_csv('Google Drive/NLP Shared task/toefl11_trainingdata_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "toefl = pd.read_csv('toefl11_trainingdata_features')\n",
    "toefl = toefl.drop(['Unnamed: 0','Unnamed: 0.1'], axis= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline\n",
    "#define features and y \n",
    "x = toefl.drop(['Filename', 'Language', 'Proficiency'], axis= 1)\n",
    "y = toefl.drop(['Filename','text','POS','DEP', 'NER', 'prop_punct', 'prop_capwords', 'prop_capI','avg_sentlength'], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define custom tranformer\n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "        \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline\n",
    "#define features and y \n",
    "x = toefl.drop(['Filename', 'Language', 'Proficiency', 'NER', 'prop_capwords'], axis= 1)\n",
    "y = toefl.drop(['Filename','text','POS','DEP', 'NER', 'prop_punct', 'prop_capwords', 'prop_capI','avg_sentlength'], axis = 1)\n",
    "\n",
    "#feature simplification\n",
    "#- NER\n",
    "# - prop_capwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = ['prop_punct', 'avg_sentlength', 'prop_capI']\n",
    "tfvect = CountVectorizer()\n",
    "tfidfvect = TfidfVectorizer()\n",
    "svm = SVC()\n",
    "linsvm = LinearSVC()\n",
    "#refit with best ranges\n",
    "numfeat_pipe = Pipeline([\n",
    "    ('selector', ItemSelector(key=numeric_features))\n",
    "])\n",
    "\n",
    "# pipe word n-grams\n",
    "text_pipe = Pipeline([\n",
    "    ('selector', ItemSelector(key='text')),\n",
    "    ('vect', TfidfVectorizer(\n",
    "        tokenizer=word_tokenize,\n",
    "        analyzer='word',\n",
    "        lowercase=True,\n",
    "        ngram_range=(1,2)))\n",
    "])\n",
    "\n",
    "# pipe char n-grams\n",
    "char_pipe = Pipeline([\n",
    "    ('selector', ItemSelector(key=\"text\")),\n",
    "    ('vect', TfidfVectorizer(\n",
    "        analyzer='char',\n",
    "        lowercase=True,\n",
    "        ngram_range=(2,3)))\n",
    "])\n",
    "\n",
    "# pipe POS n-grams\n",
    "pos_pipe = Pipeline([\n",
    "    ('selector', ItemSelector(key=\"POS\")),\n",
    "    ('vect', TfidfVectorizer(\n",
    "        analyzer='word',\n",
    "        lowercase=True,\n",
    "        ngram_range=(1,4)))\n",
    "])\n",
    "\n",
    "# pipe complete\n",
    "pipe = Pipeline([\n",
    "    ('feats', FeatureUnion([\n",
    "        ('numfeat_pipe', numfeat_pipe),\n",
    "        ('text_pipe', text_pipe),\n",
    "        ('char_pipe', char_pipe),\n",
    "        ('pos_pipe', pos_pipe)\n",
    "    ])),\n",
    "    ('cls', linsvm)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x, y['Proficiency'], test_size= 0.20,, random_state=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\robin\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('feats',\n",
       "                 FeatureUnion(transformer_list=[('numfeat_pipe',\n",
       "                                                 Pipeline(steps=[('selector',\n",
       "                                                                  ItemSelector(key=['prop_punct',\n",
       "                                                                                    'avg_sentlength',\n",
       "                                                                                    'prop_capI']))])),\n",
       "                                                ('text_pipe',\n",
       "                                                 Pipeline(steps=[('selector',\n",
       "                                                                  ItemSelector(key='text')),\n",
       "                                                                 ('vect',\n",
       "                                                                  TfidfVectorizer(ngram_range=(1,\n",
       "                                                                                               2),\n",
       "                                                                                  tokenizer=<function word_tokenize at 0x000001F51C7AFA60>))])),\n",
       "                                                ('char_pipe',\n",
       "                                                 Pipeline(steps=[('selector',\n",
       "                                                                  ItemSelector(key='text')),\n",
       "                                                                 ('vect',\n",
       "                                                                  TfidfVectorizer(analyzer='char',\n",
       "                                                                                  ngram_range=(2,\n",
       "                                                                                               3)))])),\n",
       "                                                ('pos_pipe',\n",
       "                                                 Pipeline(steps=[('selector',\n",
       "                                                                  ItemSelector(key='POS')),\n",
       "                                                                 ('vect',\n",
       "                                                                  TfidfVectorizer(ngram_range=(1,\n",
       "                                                                                               4)))]))])),\n",
       "                ('cls', LinearSVC())])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        high       0.66      0.79      0.72       783\n",
      "         low       0.73      0.33      0.46       239\n",
      "      medium       0.72      0.71      0.71      1178\n",
      "\n",
      "    accuracy                           0.70      2200\n",
      "   macro avg       0.70      0.61      0.63      2200\n",
      "weighted avg       0.70      0.70      0.69      2200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# prediction\n",
    "y_test_pred = pipe.predict(X_test)\n",
    "\n",
    "# accuracy\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -avg sent_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = toefl.drop(['Filename', 'Language', 'Proficiency', 'NER', 'prop_capwords','avg_sentlength'], axis = 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y['Proficiency'], test_size= 0.20,, random_state=11)\n",
    "# dropping avg_sent lenght as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = ['prop_punct', 'prop_capI']\n",
    "tfvect = CountVectorizer()\n",
    "tfidfvect = TfidfVectorizer()\n",
    "svm = SVC()\n",
    "linsvm = LinearSVC()\n",
    "#refit with best ranges\n",
    "numfeat_pipe = Pipeline([\n",
    "    ('selector', ItemSelector(key=numeric_features))\n",
    "])\n",
    "\n",
    "# pipe word n-grams\n",
    "text_pipe = Pipeline([\n",
    "    ('selector', ItemSelector(key='text')),\n",
    "    ('vect', TfidfVectorizer(\n",
    "        tokenizer=word_tokenize,\n",
    "        analyzer='word',\n",
    "        lowercase=True,\n",
    "        ngram_range=(1,2)))\n",
    "])\n",
    "\n",
    "# pipe char n-grams\n",
    "char_pipe = Pipeline([\n",
    "    ('selector', ItemSelector(key=\"text\")),\n",
    "    ('vect', TfidfVectorizer(\n",
    "        analyzer='char',\n",
    "        lowercase=True,\n",
    "        ngram_range=(2,3)))\n",
    "])\n",
    "\n",
    "# pipe POS n-grams\n",
    "pos_pipe = Pipeline([\n",
    "    ('selector', ItemSelector(key=\"POS\")),\n",
    "    ('vect', TfidfVectorizer(\n",
    "        analyzer='word',\n",
    "        lowercase=True,\n",
    "        ngram_range=(1,4)))\n",
    "])\n",
    "\n",
    "# pipe complete\n",
    "pipe = Pipeline([\n",
    "    ('feats', FeatureUnion([\n",
    "        ('numfeat_pipe', numfeat_pipe),\n",
    "        ('text_pipe', text_pipe),\n",
    "        ('char_pipe', char_pipe),\n",
    "        ('pos_pipe', pos_pipe)\n",
    "    ])),\n",
    "    ('cls', linsvm)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('feats',\n",
       "                 FeatureUnion(transformer_list=[('numfeat_pipe',\n",
       "                                                 Pipeline(steps=[('selector',\n",
       "                                                                  ItemSelector(key=['prop_punct',\n",
       "                                                                                    'prop_capI']))])),\n",
       "                                                ('text_pipe',\n",
       "                                                 Pipeline(steps=[('selector',\n",
       "                                                                  ItemSelector(key='text')),\n",
       "                                                                 ('vect',\n",
       "                                                                  TfidfVectorizer(ngram_range=(1,\n",
       "                                                                                               2),\n",
       "                                                                                  tokenizer=<function word_tokenize at 0x000001F51C7AFA60>))])),\n",
       "                                                ('char_pipe',\n",
       "                                                 Pipeline(steps=[('selector',\n",
       "                                                                  ItemSelector(key='text')),\n",
       "                                                                 ('vect',\n",
       "                                                                  TfidfVectorizer(analyzer='char',\n",
       "                                                                                  ngram_range=(2,\n",
       "                                                                                               3)))])),\n",
       "                                                ('pos_pipe',\n",
       "                                                 Pipeline(steps=[('selector',\n",
       "                                                                  ItemSelector(key='POS')),\n",
       "                                                                 ('vect',\n",
       "                                                                  TfidfVectorizer(ngram_range=(1,\n",
       "                                                                                               4)))]))])),\n",
       "                ('cls', LinearSVC())])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        high       0.66      0.79      0.72       761\n",
      "         low       0.82      0.36      0.50       221\n",
      "      medium       0.75      0.73      0.74      1218\n",
      "\n",
      "    accuracy                           0.72      2200\n",
      "   macro avg       0.74      0.63      0.66      2200\n",
      "weighted avg       0.73      0.72      0.71      2200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# prediction\n",
    "y_test_pred = pipe.predict(X_test)\n",
    "\n",
    "# accuracy\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -numeric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = toefl.drop(['Filename', 'Language', 'Proficiency', 'NER', 'prop_capwords','prop_capI','prop_punct','avg_sentlength'], axis = 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y['Proficiency'], test_size= 0.20, random_state=11)\n",
    "# dropping all numeric features + NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "linsvm = LinearSVC()\n",
    "#refit with best ranges\n",
    "\n",
    "# pipe word n-grams\n",
    "text_pipe = Pipeline([\n",
    "    ('selector', ItemSelector(key='text')),\n",
    "    ('vect', TfidfVectorizer(\n",
    "        tokenizer=word_tokenize,\n",
    "        analyzer='word',\n",
    "        lowercase=True,\n",
    "        ngram_range=(1,2)))\n",
    "])\n",
    "\n",
    "# pipe char n-grams\n",
    "char_pipe = Pipeline([\n",
    "    ('selector', ItemSelector(key=\"text\")),\n",
    "    ('vect', TfidfVectorizer(\n",
    "        analyzer='char',\n",
    "        lowercase=True,\n",
    "        ngram_range=(2,3)))\n",
    "])\n",
    "\n",
    "# pipe POS n-grams\n",
    "pos_pipe = Pipeline([\n",
    "    ('selector', ItemSelector(key=\"POS\")),\n",
    "    ('vect', TfidfVectorizer(\n",
    "        analyzer='word',\n",
    "        lowercase=True,\n",
    "        ngram_range=(1,4)))\n",
    "])\n",
    "\n",
    "# pipe complete\n",
    "pipe = Pipeline([\n",
    "    ('feats', FeatureUnion([\n",
    "        ('text_pipe', text_pipe),\n",
    "        ('char_pipe', char_pipe),\n",
    "        ('pos_pipe', pos_pipe)\n",
    "    ])),\n",
    "    ('cls', linsvm)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('feats',\n",
       "                 FeatureUnion(transformer_list=[('text_pipe',\n",
       "                                                 Pipeline(steps=[('selector',\n",
       "                                                                  ItemSelector(key='text')),\n",
       "                                                                 ('vect',\n",
       "                                                                  TfidfVectorizer(ngram_range=(1,\n",
       "                                                                                               2),\n",
       "                                                                                  tokenizer=<function word_tokenize at 0x000001F51C7AFA60>))])),\n",
       "                                                ('char_pipe',\n",
       "                                                 Pipeline(steps=[('selector',\n",
       "                                                                  ItemSelector(key='text')),\n",
       "                                                                 ('vect',\n",
       "                                                                  TfidfVectorizer(analyzer='char',\n",
       "                                                                                  ngram_range=(2,\n",
       "                                                                                               3)))])),\n",
       "                                                ('pos_pipe',\n",
       "                                                 Pipeline(steps=[('selector',\n",
       "                                                                  ItemSelector(key='POS')),\n",
       "                                                                 ('vect',\n",
       "                                                                  TfidfVectorizer(ngram_range=(1,\n",
       "                                                                                               4)))]))])),\n",
       "                ('cls', LinearSVC())])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        high       0.66      0.80      0.72       765\n",
      "         low       0.80      0.34      0.48       238\n",
      "      medium       0.74      0.72      0.73      1197\n",
      "\n",
      "    accuracy                           0.71      2200\n",
      "   macro avg       0.73      0.62      0.64      2200\n",
      "weighted avg       0.72      0.71      0.70      2200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# prediction\n",
    "y_test_pred = pipe.predict(X_test)\n",
    "\n",
    "# accuracy\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLI experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define features and y \n",
    "x = toefl.drop(['Filename', 'Language', 'Proficiency'], axis= 1)\n",
    "y = toefl.drop(['Filename','text','POS','DEP', 'NER', 'prop_punct', 'prop_capwords', 'prop_capI','avg_sentlength'], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = ['prop_punct', 'avg_sentlength', 'prop_capwords', 'prop_capI']\n",
    "tfidfvect = TfidfVectorizer()\n",
    "linsvm = LinearSVC()\n",
    "#refit with best ranges\n",
    "numfeat_pipe = Pipeline([\n",
    "    ('selector', ItemSelector(key=numeric_features))\n",
    "])\n",
    "\n",
    "# pipe word n-grams\n",
    "text_pipe = Pipeline([\n",
    "    ('selector', ItemSelector(key='text')),\n",
    "    ('vect', TfidfVectorizer(\n",
    "        tokenizer=word_tokenize,\n",
    "        analyzer='word',\n",
    "        lowercase=True,\n",
    "        ngram_range=(1,2)))\n",
    "])\n",
    "\n",
    "# pipe char n-grams\n",
    "char_pipe = Pipeline([\n",
    "    ('selector', ItemSelector(key=\"text\")),\n",
    "    ('vect', TfidfVectorizer(\n",
    "        analyzer='char',\n",
    "        lowercase=True,\n",
    "        ngram_range=(2,4)))\n",
    "])\n",
    "\n",
    "# pipe POS n-grams\n",
    "pos_pipe = Pipeline([\n",
    "    ('selector', ItemSelector(key=\"POS\")),\n",
    "    ('vect', TfidfVectorizer(\n",
    "        analyzer='word',\n",
    "        lowercase=True,\n",
    "        ngram_range=(1,3)))\n",
    "])\n",
    "\n",
    "# pipe complete\n",
    "pipe = Pipeline([\n",
    "    ('feats', FeatureUnion([\n",
    "        ('numfeat_pipe', numfeat_pipe),\n",
    "        ('text_pipe', text_pipe),\n",
    "        ('char_pipe', char_pipe),\n",
    "        ('pos_pipe', pos_pipe)\n",
    "    ])),\n",
    "    ('cls', linsvm)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\robin\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('feats',\n",
       "                 FeatureUnion(transformer_list=[('numfeat_pipe',\n",
       "                                                 Pipeline(steps=[('selector',\n",
       "                                                                  ItemSelector(key=['prop_punct',\n",
       "                                                                                    'avg_sentlength',\n",
       "                                                                                    'prop_capwords',\n",
       "                                                                                    'prop_capI']))])),\n",
       "                                                ('text_pipe',\n",
       "                                                 Pipeline(steps=[('selector',\n",
       "                                                                  ItemSelector(key='text')),\n",
       "                                                                 ('vect',\n",
       "                                                                  TfidfVectorizer(ngram_range=(1,\n",
       "                                                                                               2),\n",
       "                                                                                  tokenizer=<function word_tokenize at 0x0000022F4005F8B0>))])),\n",
       "                                                ('char_pipe',\n",
       "                                                 Pipeline(steps=[('selector',\n",
       "                                                                  ItemSelector(key='text')),\n",
       "                                                                 ('vect',\n",
       "                                                                  TfidfVectorizer(analyzer='char',\n",
       "                                                                                  ngram_range=(2,\n",
       "                                                                                               4)))])),\n",
       "                                                ('pos_pipe',\n",
       "                                                 Pipeline(steps=[('selector',\n",
       "                                                                  ItemSelector(key='POS')),\n",
       "                                                                 ('vect',\n",
       "                                                                  TfidfVectorizer(ngram_range=(1,\n",
       "                                                                                               3)))]))])),\n",
       "                ('cls', LinearSVC())])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x, y['Language'], test_size= 0.20, random_state=11)\n",
    "# fit model\n",
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ARA       0.83      0.76      0.79       188\n",
      "         DEU       0.88      0.91      0.89       193\n",
      "         FRA       0.83      0.77      0.80       198\n",
      "         HIN       0.68      0.69      0.68       214\n",
      "         ITA       0.89      0.83      0.86       199\n",
      "         JPN       0.77      0.74      0.76       193\n",
      "         KOR       0.74      0.79      0.76       203\n",
      "         SPA       0.74      0.78      0.76       200\n",
      "         TEL       0.71      0.74      0.72       192\n",
      "         TUR       0.83      0.84      0.84       208\n",
      "         ZHO       0.84      0.85      0.84       212\n",
      "\n",
      "    accuracy                           0.79      2200\n",
      "   macro avg       0.79      0.79      0.79      2200\n",
      "weighted avg       0.79      0.79      0.79      2200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# prediction\n",
    "y_test_pred = pipe.predict(X_test)\n",
    "\n",
    "# accuracy\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping NER & prop capwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline\n",
    "#define features and y \n",
    "x = toefl.drop(['Filename', 'Language', 'Proficiency', 'NER', 'prop_capwords'], axis= 1)\n",
    "#feature simplification\n",
    "#- NER\n",
    "# - prop_capwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = ['prop_punct', 'avg_sentlength', 'prop_capI']\n",
    "tfidfvect = TfidfVectorizer()\n",
    "linsvm = LinearSVC()\n",
    "#refit with best ranges\n",
    "numfeat_pipe = Pipeline([\n",
    "    ('selector', ItemSelector(key=numeric_features))\n",
    "])\n",
    "\n",
    "# pipe word n-grams\n",
    "text_pipe = Pipeline([\n",
    "    ('selector', ItemSelector(key='text')),\n",
    "    ('vect', TfidfVectorizer(\n",
    "        tokenizer=word_tokenize,\n",
    "        analyzer='word',\n",
    "        lowercase=True,\n",
    "        ngram_range=(1,2)))\n",
    "])\n",
    "\n",
    "# pipe char n-grams\n",
    "char_pipe = Pipeline([\n",
    "    ('selector', ItemSelector(key=\"text\")),\n",
    "    ('vect', TfidfVectorizer(\n",
    "        analyzer='char',\n",
    "        lowercase=True,\n",
    "        ngram_range=(2,4)))\n",
    "])\n",
    "\n",
    "# pipe POS n-grams\n",
    "pos_pipe = Pipeline([\n",
    "    ('selector', ItemSelector(key=\"POS\")),\n",
    "    ('vect', TfidfVectorizer(\n",
    "        analyzer='word',\n",
    "        lowercase=True,\n",
    "        ngram_range=(1,3)))\n",
    "])\n",
    "\n",
    "# pipe complete\n",
    "pipe = Pipeline([\n",
    "    ('feats', FeatureUnion([\n",
    "        ('numfeat_pipe', numfeat_pipe),\n",
    "        ('text_pipe', text_pipe),\n",
    "        ('char_pipe', char_pipe),\n",
    "        ('pos_pipe', pos_pipe)\n",
    "    ])),\n",
    "    ('cls', linsvm)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('feats',\n",
       "                 FeatureUnion(transformer_list=[('text_pipe',\n",
       "                                                 Pipeline(steps=[('selector',\n",
       "                                                                  ItemSelector(key='text')),\n",
       "                                                                 ('vect',\n",
       "                                                                  TfidfVectorizer(ngram_range=(1,\n",
       "                                                                                               2),\n",
       "                                                                                  tokenizer=<function word_tokenize at 0x0000022F4005F8B0>))])),\n",
       "                                                ('char_pipe',\n",
       "                                                 Pipeline(steps=[('selector',\n",
       "                                                                  ItemSelector(key='text')),\n",
       "                                                                 ('vect',\n",
       "                                                                  TfidfVectorizer(analyzer='char',\n",
       "                                                                                  ngram_range=(2,\n",
       "                                                                                               3)))])),\n",
       "                                                ('pos_pipe',\n",
       "                                                 Pipeline(steps=[('selector',\n",
       "                                                                  ItemSelector(key='POS')),\n",
       "                                                                 ('vect',\n",
       "                                                                  TfidfVectorizer(ngram_range=(1,\n",
       "                                                                                               4)))]))])),\n",
       "                ('cls', LinearSVC())])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x, y['Language'], test_size= 0.20, random_state=11)\n",
    "# fit model\n",
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ARA       0.85      0.76      0.80       196\n",
      "         DEU       0.86      0.92      0.89       194\n",
      "         FRA       0.81      0.81      0.81       206\n",
      "         HIN       0.69      0.77      0.73       192\n",
      "         ITA       0.84      0.85      0.85       218\n",
      "         JPN       0.84      0.78      0.80       196\n",
      "         KOR       0.76      0.79      0.78       188\n",
      "         SPA       0.74      0.77      0.76       203\n",
      "         TEL       0.79      0.74      0.76       204\n",
      "         TUR       0.89      0.85      0.87       217\n",
      "         ZHO       0.85      0.89      0.87       186\n",
      "\n",
      "    accuracy                           0.81      2200\n",
      "   macro avg       0.81      0.81      0.81      2200\n",
      "weighted avg       0.81      0.81      0.81      2200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# prediction\n",
    "y_test_pred = pipe.predict(X_test)\n",
    "\n",
    "# accuracy\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -avg sent_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = toefl.drop(['Filename', 'Language', 'Proficiency', 'NER', 'prop_capwords','avg_sentlength'], axis = 1, random_state=11)\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y['Language'], test_size= 0.20)\n",
    "# dropping avg_sent lenght as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = ['prop_punct', 'prop_capI']\n",
    "tfvect = CountVectorizer()\n",
    "tfidfvect = TfidfVectorizer()\n",
    "svm = SVC()\n",
    "linsvm = LinearSVC()\n",
    "#refit with best ranges\n",
    "numfeat_pipe = Pipeline([\n",
    "    ('selector', ItemSelector(key=numeric_features))\n",
    "])\n",
    "\n",
    "# pipe word n-grams\n",
    "text_pipe = Pipeline([\n",
    "    ('selector', ItemSelector(key='text')),\n",
    "    ('vect', TfidfVectorizer(\n",
    "        tokenizer=word_tokenize,\n",
    "        analyzer='word',\n",
    "        lowercase=True,\n",
    "        ngram_range=(1,2)))\n",
    "])\n",
    "\n",
    "# pipe char n-grams\n",
    "char_pipe = Pipeline([\n",
    "    ('selector', ItemSelector(key=\"text\")),\n",
    "    ('vect', TfidfVectorizer(\n",
    "        analyzer='char',\n",
    "        lowercase=True,\n",
    "        ngram_range=(2,3)))\n",
    "])\n",
    "\n",
    "# pipe POS n-grams\n",
    "pos_pipe = Pipeline([\n",
    "    ('selector', ItemSelector(key=\"POS\")),\n",
    "    ('vect', TfidfVectorizer(\n",
    "        analyzer='word',\n",
    "        lowercase=True,\n",
    "        ngram_range=(1,4)))\n",
    "])\n",
    "\n",
    "# pipe complete\n",
    "pipe = Pipeline([\n",
    "    ('feats', FeatureUnion([\n",
    "        ('numfeat_pipe', numfeat_pipe),\n",
    "        ('text_pipe', text_pipe),\n",
    "        ('char_pipe', char_pipe),\n",
    "        ('pos_pipe', pos_pipe)\n",
    "    ])),\n",
    "    ('cls', linsvm)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('feats',\n",
       "                 FeatureUnion(transformer_list=[('numfeat_pipe',\n",
       "                                                 Pipeline(steps=[('selector',\n",
       "                                                                  ItemSelector(key=['prop_punct',\n",
       "                                                                                    'prop_capI']))])),\n",
       "                                                ('text_pipe',\n",
       "                                                 Pipeline(steps=[('selector',\n",
       "                                                                  ItemSelector(key='text')),\n",
       "                                                                 ('vect',\n",
       "                                                                  TfidfVectorizer(ngram_range=(1,\n",
       "                                                                                               2),\n",
       "                                                                                  tokenizer=<function word_tokenize at 0x0000022F4005F8B0>))])),\n",
       "                                                ('char_pipe',\n",
       "                                                 Pipeline(steps=[('selector',\n",
       "                                                                  ItemSelector(key='text')),\n",
       "                                                                 ('vect',\n",
       "                                                                  TfidfVectorizer(analyzer='char',\n",
       "                                                                                  ngram_range=(2,\n",
       "                                                                                               3)))])),\n",
       "                                                ('pos_pipe',\n",
       "                                                 Pipeline(steps=[('selector',\n",
       "                                                                  ItemSelector(key='POS')),\n",
       "                                                                 ('vect',\n",
       "                                                                  TfidfVectorizer(ngram_range=(1,\n",
       "                                                                                               4)))]))])),\n",
       "                ('cls', LinearSVC())])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ARA       0.88      0.74      0.81       213\n",
      "         DEU       0.86      0.92      0.89       200\n",
      "         FRA       0.81      0.78      0.79       192\n",
      "         HIN       0.70      0.74      0.72       200\n",
      "         ITA       0.86      0.85      0.86       200\n",
      "         JPN       0.86      0.73      0.79       188\n",
      "         KOR       0.69      0.80      0.74       178\n",
      "         SPA       0.73      0.75      0.74       204\n",
      "         TEL       0.77      0.75      0.76       203\n",
      "         TUR       0.84      0.88      0.86       205\n",
      "         ZHO       0.84      0.85      0.85       217\n",
      "\n",
      "    accuracy                           0.80      2200\n",
      "   macro avg       0.80      0.80      0.80      2200\n",
      "weighted avg       0.80      0.80      0.80      2200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# prediction\n",
    "y_test_pred = pipe.predict(X_test)\n",
    "\n",
    "# accuracy\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -numeric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = toefl.drop(['Filename', 'Language', 'Proficiency', 'NER', 'prop_capwords','prop_capI','prop_punct','avg_sentlength'], axis = 1, random_state=11)\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y['Language'], test_size= 0.20)\n",
    "# dropping all numeric features + NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "linsvm = LinearSVC()\n",
    "#refit with best ranges\n",
    "\n",
    "# pipe word n-grams\n",
    "text_pipe = Pipeline([\n",
    "    ('selector', ItemSelector(key='text')),\n",
    "    ('vect', TfidfVectorizer(\n",
    "        tokenizer=word_tokenize,\n",
    "        analyzer='word',\n",
    "        lowercase=True,\n",
    "        ngram_range=(1,2)))\n",
    "])\n",
    "\n",
    "# pipe char n-grams\n",
    "char_pipe = Pipeline([\n",
    "    ('selector', ItemSelector(key=\"text\")),\n",
    "    ('vect', TfidfVectorizer(\n",
    "        analyzer='char',\n",
    "        lowercase=True,\n",
    "        ngram_range=(2,3)))\n",
    "])\n",
    "\n",
    "# pipe POS n-grams\n",
    "pos_pipe = Pipeline([\n",
    "    ('selector', ItemSelector(key=\"POS\")),\n",
    "    ('vect', TfidfVectorizer(\n",
    "        analyzer='word',\n",
    "        lowercase=True,\n",
    "        ngram_range=(1,4)))\n",
    "])\n",
    "\n",
    "# pipe complete\n",
    "pipe = Pipeline([\n",
    "    ('feats', FeatureUnion([\n",
    "        ('text_pipe', text_pipe),\n",
    "        ('char_pipe', char_pipe),\n",
    "        ('pos_pipe', pos_pipe)\n",
    "    ])),\n",
    "    ('cls', linsvm)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('feats',\n",
       "                 FeatureUnion(transformer_list=[('text_pipe',\n",
       "                                                 Pipeline(steps=[('selector',\n",
       "                                                                  ItemSelector(key='text')),\n",
       "                                                                 ('vect',\n",
       "                                                                  TfidfVectorizer(ngram_range=(1,\n",
       "                                                                                               2),\n",
       "                                                                                  tokenizer=<function word_tokenize at 0x0000022F4005F8B0>))])),\n",
       "                                                ('char_pipe',\n",
       "                                                 Pipeline(steps=[('selector',\n",
       "                                                                  ItemSelector(key='text')),\n",
       "                                                                 ('vect',\n",
       "                                                                  TfidfVectorizer(analyzer='char',\n",
       "                                                                                  ngram_range=(2,\n",
       "                                                                                               3)))])),\n",
       "                                                ('pos_pipe',\n",
       "                                                 Pipeline(steps=[('selector',\n",
       "                                                                  ItemSelector(key='POS')),\n",
       "                                                                 ('vect',\n",
       "                                                                  TfidfVectorizer(ngram_range=(1,\n",
       "                                                                                               4)))]))])),\n",
       "                ('cls', LinearSVC())])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ARA       0.86      0.78      0.82       198\n",
      "         DEU       0.87      0.94      0.90       191\n",
      "         FRA       0.86      0.84      0.85       206\n",
      "         HIN       0.69      0.72      0.71       199\n",
      "         ITA       0.85      0.84      0.85       212\n",
      "         JPN       0.79      0.74      0.76       205\n",
      "         KOR       0.73      0.76      0.74       182\n",
      "         SPA       0.76      0.77      0.77       217\n",
      "         TEL       0.78      0.75      0.76       216\n",
      "         TUR       0.87      0.87      0.87       183\n",
      "         ZHO       0.80      0.86      0.83       191\n",
      "\n",
      "    accuracy                           0.80      2200\n",
      "   macro avg       0.81      0.81      0.81      2200\n",
      "weighted avg       0.81      0.80      0.80      2200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# prediction\n",
    "y_test_pred = pipe.predict(X_test)\n",
    "\n",
    "# accuracy\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### only dropping NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = toefl.drop(['Filename', 'Language', 'Proficiency','NER'], axis = 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y['Language'], test_size= 0.20, random_state=11)\n",
    "# only dropping NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = ['prop_punct', 'avg_sentlength', 'prop_capwords', 'prop_capI']\n",
    "tfvect = CountVectorizer()\n",
    "tfidfvect = TfidfVectorizer()\n",
    "svm = SVC()\n",
    "linsvm = LinearSVC()\n",
    "#refit with best ranges\n",
    "numfeat_pipe = Pipeline([\n",
    "    ('selector', ItemSelector(key=numeric_features))\n",
    "])\n",
    "\n",
    "# pipe word n-grams\n",
    "text_pipe = Pipeline([\n",
    "    ('selector', ItemSelector(key='text')),\n",
    "    ('vect', TfidfVectorizer(\n",
    "        tokenizer=word_tokenize,\n",
    "        analyzer='word',\n",
    "        lowercase=True,\n",
    "        ngram_range=(1,2)))\n",
    "])\n",
    "\n",
    "# pipe char n-grams\n",
    "char_pipe = Pipeline([\n",
    "    ('selector', ItemSelector(key=\"text\")),\n",
    "    ('vect', TfidfVectorizer(\n",
    "        analyzer='char',\n",
    "        lowercase=True,\n",
    "        ngram_range=(2,3)))\n",
    "])\n",
    "\n",
    "# pipe POS n-grams\n",
    "pos_pipe = Pipeline([\n",
    "    ('selector', ItemSelector(key=\"POS\")),\n",
    "    ('vect', TfidfVectorizer(\n",
    "        analyzer='word',\n",
    "        lowercase=True,\n",
    "        ngram_range=(1,4)))\n",
    "])\n",
    "\n",
    "# pipe complete\n",
    "pipe = Pipeline([\n",
    "    ('feats', FeatureUnion([\n",
    "        ('numfeat_pipe', numfeat_pipe),\n",
    "        ('text_pipe', text_pipe),\n",
    "        ('char_pipe', char_pipe),\n",
    "        ('pos_pipe', pos_pipe)\n",
    "    ])),\n",
    "    ('cls', linsvm)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\robin\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('feats',\n",
       "                 FeatureUnion(transformer_list=[('numfeat_pipe',\n",
       "                                                 Pipeline(steps=[('selector',\n",
       "                                                                  ItemSelector(key=['prop_punct',\n",
       "                                                                                    'avg_sentlength',\n",
       "                                                                                    'prop_capwords',\n",
       "                                                                                    'prop_capI']))])),\n",
       "                                                ('text_pipe',\n",
       "                                                 Pipeline(steps=[('selector',\n",
       "                                                                  ItemSelector(key='text')),\n",
       "                                                                 ('vect',\n",
       "                                                                  TfidfVectorizer(ngram_range=(1,\n",
       "                                                                                               2),\n",
       "                                                                                  tokenizer=<function word_tokenize at 0x0000022F4005F8B0>))])),\n",
       "                                                ('char_pipe',\n",
       "                                                 Pipeline(steps=[('selector',\n",
       "                                                                  ItemSelector(key='text')),\n",
       "                                                                 ('vect',\n",
       "                                                                  TfidfVectorizer(analyzer='char',\n",
       "                                                                                  ngram_range=(2,\n",
       "                                                                                               3)))])),\n",
       "                                                ('pos_pipe',\n",
       "                                                 Pipeline(steps=[('selector',\n",
       "                                                                  ItemSelector(key='POS')),\n",
       "                                                                 ('vect',\n",
       "                                                                  TfidfVectorizer(ngram_range=(1,\n",
       "                                                                                               4)))]))])),\n",
       "                ('cls', LinearSVC())])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ARA       0.81      0.73      0.77       188\n",
      "         DEU       0.87      0.90      0.88       193\n",
      "         FRA       0.86      0.79      0.82       198\n",
      "         HIN       0.69      0.69      0.69       214\n",
      "         ITA       0.86      0.86      0.86       199\n",
      "         JPN       0.78      0.78      0.78       193\n",
      "         KOR       0.78      0.79      0.78       203\n",
      "         SPA       0.70      0.78      0.74       200\n",
      "         TEL       0.71      0.76      0.73       192\n",
      "         TUR       0.89      0.82      0.85       208\n",
      "         ZHO       0.85      0.85      0.85       212\n",
      "\n",
      "    accuracy                           0.79      2200\n",
      "   macro avg       0.80      0.79      0.80      2200\n",
      "weighted avg       0.80      0.79      0.80      2200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# prediction\n",
    "y_test_pred = pipe.predict(X_test)\n",
    "\n",
    "# accuracy\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping prop capwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline\n",
    "#define features and y \n",
    "x = toefl.drop(['Filename', 'Language', 'Proficiency', 'prop_capwords'], axis= 1)\n",
    "#feature simplification\n",
    "# - prop_capwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = ['prop_punct', 'avg_sentlength', 'prop_capI']\n",
    "tfidfvect = TfidfVectorizer()\n",
    "linsvm = LinearSVC()\n",
    "#refit with best ranges\n",
    "numfeat_pipe = Pipeline([\n",
    "    ('selector', ItemSelector(key=numeric_features))\n",
    "])\n",
    "\n",
    "# pipe word n-grams\n",
    "text_pipe = Pipeline([\n",
    "    ('selector', ItemSelector(key='text')),\n",
    "    ('vect', TfidfVectorizer(\n",
    "        tokenizer=word_tokenize,\n",
    "        analyzer='word',\n",
    "        lowercase=True,\n",
    "        ngram_range=(1,2)))\n",
    "])\n",
    "\n",
    "# pipe char n-grams\n",
    "char_pipe = Pipeline([\n",
    "    ('selector', ItemSelector(key=\"text\")),\n",
    "    ('vect', TfidfVectorizer(\n",
    "        analyzer='char',\n",
    "        lowercase=True,\n",
    "        ngram_range=(2,4)))\n",
    "])\n",
    "\n",
    "# pipe POS n-grams\n",
    "pos_pipe = Pipeline([\n",
    "    ('selector', ItemSelector(key=\"POS\")),\n",
    "    ('vect', TfidfVectorizer(\n",
    "        analyzer='word',\n",
    "        lowercase=True,\n",
    "        ngram_range=(1,3)))\n",
    "])\n",
    "\n",
    "# pipe complete\n",
    "pipe = Pipeline([\n",
    "    ('feats', FeatureUnion([\n",
    "        ('numfeat_pipe', numfeat_pipe),\n",
    "        ('text_pipe', text_pipe),\n",
    "        ('char_pipe', char_pipe),\n",
    "        ('pos_pipe', pos_pipe)\n",
    "    ])),\n",
    "    ('cls', linsvm)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\robin\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('feats',\n",
       "                 FeatureUnion(transformer_list=[('numfeat_pipe',\n",
       "                                                 Pipeline(steps=[('selector',\n",
       "                                                                  ItemSelector(key=['prop_punct',\n",
       "                                                                                    'avg_sentlength',\n",
       "                                                                                    'prop_capI']))])),\n",
       "                                                ('text_pipe',\n",
       "                                                 Pipeline(steps=[('selector',\n",
       "                                                                  ItemSelector(key='text')),\n",
       "                                                                 ('vect',\n",
       "                                                                  TfidfVectorizer(ngram_range=(1,\n",
       "                                                                                               2),\n",
       "                                                                                  tokenizer=<function word_tokenize at 0x0000022F4005F8B0>))])),\n",
       "                                                ('char_pipe',\n",
       "                                                 Pipeline(steps=[('selector',\n",
       "                                                                  ItemSelector(key='text')),\n",
       "                                                                 ('vect',\n",
       "                                                                  TfidfVectorizer(analyzer='char',\n",
       "                                                                                  ngram_range=(2,\n",
       "                                                                                               4)))])),\n",
       "                                                ('pos_pipe',\n",
       "                                                 Pipeline(steps=[('selector',\n",
       "                                                                  ItemSelector(key='POS')),\n",
       "                                                                 ('vect',\n",
       "                                                                  TfidfVectorizer(ngram_range=(1,\n",
       "                                                                                               3)))]))])),\n",
       "                ('cls', LinearSVC())])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x, y['Language'], test_size= 0.20, random_state=11)\n",
    "# fit model\n",
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ARA       0.83      0.76      0.79       188\n",
      "         DEU       0.88      0.90      0.89       193\n",
      "         FRA       0.84      0.78      0.81       198\n",
      "         HIN       0.66      0.73      0.69       214\n",
      "         ITA       0.88      0.84      0.86       199\n",
      "         JPN       0.77      0.76      0.76       193\n",
      "         KOR       0.76      0.77      0.77       203\n",
      "         SPA       0.74      0.78      0.76       200\n",
      "         TEL       0.71      0.73      0.72       192\n",
      "         TUR       0.88      0.82      0.85       208\n",
      "         ZHO       0.83      0.86      0.85       212\n",
      "\n",
      "    accuracy                           0.79      2200\n",
      "   macro avg       0.80      0.79      0.79      2200\n",
      "weighted avg       0.80      0.79      0.79      2200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# prediction\n",
    "y_test_pred = pipe.predict(X_test)\n",
    "\n",
    "# accuracy\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
